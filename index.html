<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Transformers</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/black.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

</head>
<body>
	<!--
	The plan:
		1. The problem with RNNs (reminder)
		2. First look at vanilla transformer
		3. Self Attention
		4. Transformer Block
		5. Attention is all you need vanilla transformer
		6. Encoder only 
		7. Examples
			7.1 Basic classification
			7.2 Bert 
	-->

	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Transformer</h2>
				<p>Architecture overview</p>
				<small>Bartosz WÃ³jtowicz</small>
			</section>

			<section>
				<h2>Prerequisites</h2>
				<ul>
					<li class="fragment">You know how Neural Networks work</li>
					<li class="fragment">Basic knowledge of PyTorch</li>
					<li class="fragment">(optional) Basic understanding of attention mechanism</li>
				</ul>
			</section>

			<section data-auto-animate>
				<h2 data-id='code-title'>Presentation outline</h2>
				<ol>
					<li class = "fragment">The problem with RNNs (quick recap)</li>
					<li class = "fragment">Quick look at Vanilla Transformer - "Attention is All You Need" paper</li>
					<li class = "fragment">Vanilla Transformer - architecture breakdown</li>
					<li class = "fragment">Examples of architectures</li>
				</ol>
			</section>

			<section>
				<h3>The problem with RNNs</h3>
				<img src = "assets/recurrent-connection.svg" style="height: 10%;">
			</section>

			<section>
				<h3>RNN problems</h3>
				<ul>
					<li>Not parallelizable within a sequence - RNNs process tokens sequentially</li>
					<li>Problems with (very) long dependencies</li>
					<li>Vanishing/exploding gradients</li>
					<li>Computationally expensive to train</li>
				</ul>
			</section>

			<section>
				<h3>Vanilla Transformer</h3>
				<div style="display: inline-grid; grid-template-columns: 50% 50%;margin-left: 0%; margin-right: 0%;">
                    <div style="height: 85%; margin-bottom: 0%;">
                        <img src="assets/vanilla_transformer.png">
                    </div>
                    <div>
						<p>We'll break it down into:</p>
						<ol>
							<li class = "fragment">Self-Attention</li>
							<li class = "fragment">Multi-Head Attention</li>
							<li class = "fragment">Position encodings and embeddings</li>
							<li class = "fragment">Encoder Block</li>
							<li class = "fragment">Decoder Block</li>
						</ol>
                    </div>
                </div>
			</section>

			<section>
				<section>	
					<h3>Self-Attention - basic example</h3>
					<p>
						We are transforming input vectors $x_1, x_2,..., x_t$ into corrseponding  output vectors $y_1, y_2,..., y_t$.
						Here $y_i$ is simply weighed avereage over all input vectors.
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div style="height: 120%; margin-right: 0%;">
							<img src="assets/self-attention-idea.svg">
						</div>
						<div>
							<p>
								$y_i = \sum_{j} w_{ij}x_j$ <br><br>
								$w'_{ij} = x_{i}^{T}x_j$ <br><br>
								$w_{ij} = \text{softmax}(w'_{ij})$
							</p>
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - key, value, query</h3>
					<p style="text-align: left">
						From each input vector $x_i$ we derive three new vectors called $key, value$ and $query$. <br>					
					</p>
					<div style="display: inline-grid; grid-template-columns: 60% 40%; margin-left: 0%; margin-right: 0%;">
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/key-query-value.svg">
						</div>
						<div>
							$k_i = W_kx_i$ <br> $v_i = W_vx_i$ <br> $q_i = W_qx_i$ <br><br>
							$y_i = \sum_{j} w_{ij}x_j$ <br><br>
							$w'_{ij} = q_{i}^{T}k_j$ <br>
							$w_{ij} = \text{softmax}(w'_{ij})$
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - Scaling dot product</h3>
					<p>
						The next trick is to scale our dot product by $\frac{1}{\sqrt{d_k}}$, where $d_k$ is the length of 
						our embedding vector. 
					</p><br>
					$y_i = \sum_{j} w_{ij}x_j$ <br>
					$w'_{ij} = \frac{q_{i}^{T}k_j}{\sqrt{d_k}}$ <br>
					$w_{ij} = \text{softmax}(w'_{ij})$
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div>
							<p>
								Keys, values and queries are packed into matrices $K, V, Q$, which 
								enables the attention function to be computed simultaneously over set of queries.
							</p>
						</div>
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/scaled-dotprod-attention.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<pre data-id="code-animation"><code class="python" data-trim data-line-numbers="1-8|10|11-13|14-15|16">
class SelfAttention(nn.Module):
  def __init__(self, emb_dim):
	super().__init__()	
	self.emb_dim = emb_dim

	self.tokeys = nn.Linear(emb_dim, emb_dim, bias=False)
	self.toqueries = nn.Linear(emb_dim, emb_dim, bias=False)
	self.tovalues = nn.Linear(emb_dim, emb_dim, bias=False)	
  def forward(self, x):
	b, t, dk = x.size()
	queries = self.toqueries(x)
	keys    = self.tokeys(x)
	values  = self.tovalues(x)		
	dot = torch.einsum("bqe, bke -> bqk", [queries, keys])
	softmaxed = torch.softmax(dot / (dk ** (1/2)), dim = 2)
	out = torch.einsum("bqk, bke -> bqe", [softmaxed, values])	
	return out					
					</code></pre>
				</section>
			</section>
			
			<section>
				<section>
					<h3>Multi-Head Attention</h3>
					<p class = "fragment">We just stack several "Scaled Dot-Product Attention" :)</p>
					<p class = "fragment" style="text-align: center">
						Each attention mechanism (indexed by $h$) with different $W_{k}^{h}$, $W_{v}^{h}$, $W^s_{h}$ matrices. 
					</p>
					<p class = "fragment" style="text-align: center">
						For input $x_i$, we produce different output vector $y_i^s$ for each head. 
					</p>
					<p class = "fragment" style="text-align: center">
						We concatenate vectors $y_i^s$ and transform them linearly back to expected dimension.
					</p>
				</section>

				<section>
					<h3>Multi-Head Attention</h3>
					<p>	
						$$\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2,..., \text{head}_h)W_o$$
						$$\text{head}_i = Attention(Q^i, K^i, V^i) $$
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div>
							<br>
							$ Q^i = XW_q^i $ <br> $ K^i = XW_k^i $ <br> $ V^i = XW_v^i $
						</div>
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/multi-head-attention.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Multi-Head Attention</h3>
					<pre data-id="code-animation">
						<code class="python" data-trim data-line-numbers="1-10|11|12-14|15-17|18-19|20|21">
class MultiHeadSelfAttention(nn.Module):
  def __init__(self, emb_dim, heads = 8):
	super().__init__()	
	self.emb_dim = emb_dim
	self.heads = heads
	
	# we combine all heads into single linear transformation
	self.tokeys = nn.Linear(emb_dim, emb_dim * 8, bias=False)
	self.toqueries = nn.Linear(emb_dim, emb_dim * 8, bias=False)
	self.tovalues = nn.Linear(emb_dim, emb_dim * 8 , bias=False)
	self.unifyheads = nn.Linear(emb_dim * 8, emb_dim)	
  def forward(self, x):
	b, t, dk = x.size()
	h = self.heads
	queries = self.toqueries(x).view(b, t, h, dk)
	keys    = self.tokeys(x).view(b, t, h, dk)
	values  = self.tovalues(x).view(b, t, h, dk)	
	dot = torch.einsum("bqhe,bkhe -> bhqk", [queries, keys])
	softmaxed = torch.softmax(dot / (dk ** (1/2)), dim = 3)
	out = torch.einsum("bhqk, bkhe -> bqhe", [softmaxed, values])	
	return self.unifyheads(out)
						</code>
					</pre>
				</section>
			</section>

			<section>
				<section>
					<h3>Position information</h3>
					<p class="fragment">
						Each layer in our model is permutation invariant (no recurrence, no convolution).
						If we change the order of words in a sentence, we get the exact same output.
					</p>
					<p class="fragment">We want out model to make use of the order of the sequence.</p>
				</section>

				<section>
					<h3>Positional Embedding</h3>
					<!-- <p>We simply embed the positions exactly the same way as words.</p> -->
					<p class="fragment">
						Just as we create (learnable) embeddings $v_{\text{cat}}, v_{\text{dog}}$ for words,
						we also create embeddings $v_{1},...,v_{m}$ for each position in a sentence.
					</p>
					<br>
					<p class="fragment">+ easy to implement, works well</p>
					<p class="fragment">- need sequences of every length during training</p>				
				</section>

				<section>
					<h3>Positional Encoding</h3>
					<p class="fragment">
						We use function $f: N \to \mathbb{R}^K$ (usually a combination of $sin$ and $cos$ functions) to create positional embedding for different positions.
					</p>
					<br>
					<p class="fragment">+ works just as well as embeddings</p>
					<p class="fragment">+ deals with sequences longer than in training</p>
					<p class="fragment">- a little harder to implement</p>
					<p class="fragment">- choice of encoding functions can be hard</p>
				</section>
			</section>

			<section>
				<section>
					<h3>Encoder Block</h3>
					<div style="display: inline-grid; grid-template-columns: 60% 40%; margin-left: 0%; margin-right: 0%;">
						<div>
							<ul><br>
								<li class="fragment">Multi-Head Self-Attention</li>
								<li class="fragment">Feed Forward layer - single FC NN applied independently to each vector</li>
								<li class="fragment">Residual connections (added before normalization)</li>
								<li class="fragment">Layer normalization layers</li>
							</ul>
						</div>
						<div style="height: 80%; margin-right: 0%;">
							<img src="assets/encoder.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Let's see it in code!</h3>
				</section>

				<section>
					<h2>Encoder code</h2>
					<pre data-id="code-animation">
						<code class="python" data-trim data-line-numbers="1-4|5|6-7|8-13|14|15|16|17|18">
class Encoder(nn.Module):

    def __init__(self, emb, heads, hidden_multiplier = 4):
        super().__init__()
        self.attention = MultiHeadSelfAttention(emb, heads=heads)
        self.norm1 = nn.LayerNorm(emb)
        self.norm2 = nn.LayerNorm(emb)
        self.ff = nn.Sequential(
            nn.Linear(emb, hidden_multiplier * emb),
            nn.ReLU(),
            nn.Linear(hidden_multiplier * emb, emb)
            )

    def forward(self, x): 
        attended = self.attention(x)
        x = self.norm1(attended + x)
        forwarded = self.ff(x)
        return self.norm2(forwarded + x)
						</code>
					</pre>
				</section>
			</section>

			<section>
				<h3>Decoder</h3>
				<div style="display: inline-grid; grid-template-columns: 40% 60%;margin-left: 0%; margin-right: 0%;">
                    <div style="height: 100%; margin-bottom: 0%;">
                        <img src="assets/vanilla_transformer.png">
                    </div>
                    <div>
						<ul>
							<li class="fragment">Encoder (stack of N) maps the input sequence into a single latent vector.</li>
							<li class="fragment">We use Masked Attention so we can only 'attend' to previous words.</li>
							<li class="fragment">In second attention we use Keys and Values from encoder, and Queries from decoder.</li>
						</ul>
                    </div>
                </div>
			</section>

			</div>
	</div>
	
	<script src="plugin/math/math.js"></script>
	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath  ]
		});
	</script>
</body>
</html>
