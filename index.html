<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Transformers</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/black.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Transformer</h2>
				<p>Architecture overview</p>
				<small>Bartosz WÃ³jtowicz</small>
			</section>

			<section>
				<h2>Prerequisites</h2>
				<ul>
					<li class="fragment">You know how Neural Networks work</li>
					<li class="fragment">Basic knowledge of PyTorch</li>
					<li class="fragment">(optional) Basic understanding of attention mechanism</li>
				</ul>
			</section>

		<!-- Gonna skip those, no need 

			<section>
				<h3>The problem with RNNs (Recap)</h3>
				<img src = "assets/recurrent-connection.svg" style="height: 10%;">
			</section>

			<section>
				<h3>RNN problems</h3>
				<ul>
					<li>Not parallelizable within a sequence - RNNs process tokens sequentially</li>
					<li>Problems with (very) long dependencies</li>
					<li>Vanishing/exploding gradients</li>
					<li>Computationally expensive to train</li>
				</ul>
			</section>

		-->

			<section>
				<h3>Vanilla Transformer - Quick Look</h3>
				<div style="display: inline-grid; grid-template-columns: 40% 60%;margin-left: 0%; margin-right: 0%;">
					<div style="height: 100%; margin-bottom: 0%;">
						<img src="assets/vanilla_transformer.png">
					</div>
					<div>
						<ul>
							<br>
							<li class = "fragment">Encoder - maps input sequence into a latent representation (memory) of the whole sequence.</li><br>
							<li class = "fragment">Decoder - generates output based on memory and words already generated. </li>
						</ul>
					</div>
                </div>
			</section>

			<section>
				<h3>Simpler Transformers</h3>
				<p class="fragment">Modern Architectures (e.g. BERT, GPT2) are encoder-only architectures, thus they became much more simpler.</p>
				<p class="fragment">In this presentation we will mostly focus on those architectures.</p>
			</section>

			<section data-auto-animate>
				<h2 data-id='code-title'>Presentation outline</h2>
				<ol>
					<li class = "fragment">Self-Attention</li>
					<li class = "fragment">Multi-Head Scaled Dot-Product Attention</li>
					<li class = "fragment">Positional Embeddings/Encodings</li>
					<li class = "fragment">Encoder Block</li>
					<li class = "fragment">Architecture examples</li>
				</ol>
			</section>

			<section>
				<section>	
					<h3>Self-Attention - basic example</h3>
					<p>
						We are transforming input vectors $x_1, x_2,..., x_t$ into corrseponding  output vectors $y_1, y_2,..., y_t$. <br>
						E.g. $y_i$ can be weighed avereage over all input vectors.
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;" class="fragment">
						<div style="height: 120%; margin-right: 0%;">
							<img src="assets/self-attention-idea.svg">
						</div>
						<div>
							<p>
								$y_i = \sum_{j} w_{ij}x_j$ <br><br>
								$w'_{ij} = x_{i}^{T}x_j$ <br><br>
								$w_{ij} = \text{softmax}(w'_{ij})$
							</p>
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - key, value, query</h3>
					<p style="text-align: left">
						From each input vector $x_i$ we derive three new vectors called $key, value$ and $query$. <br>					
					</p>
					<div style="display: inline-grid; grid-template-columns: 60% 40%; margin-left: 0%; margin-right: 0%;">
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/key-query-value.svg">
						</div>
						<div>
							$k_i = W_kx_i$ <br> $v_i = W_vx_i$ <br> $q_i = W_qx_i$ <br><br>
							$y_i = \sum_{j} w_{ij}x_j$ <br><br>
							$w'_{ij} = q_{i}^{T}k_j$ <br>
							$w_{ij} = \text{softmax}(w'_{ij})$
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - Scaling dot product</h3>
					<p>
						The next trick is to scale our dot product by $\frac{1}{\sqrt{d_k}}$, where $d_k$ is the length of 
						our embedding vector. 
					</p><br>
					$y_i = \sum_{j} w_{ij}x_j$ <br>
					$w'_{ij} = \frac{q_{i}^{T}k_j}{\sqrt{d_k}}$ <br>
					$w_{ij} = \text{softmax}(w'_{ij})$
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div>
							<p>
								Keys, values and queries are packed into matrices $K, V, Q$, which 
								enables the attention function to be computed simultaneously over set of queries.
							</p>
						</div>
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/scaled-dotprod-attention.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<pre data-id="code-animation"><code class="python" data-trim data-line-numbers="1-8|10|11-13|14-15|16">
class SelfAttention(nn.Module):
  def __init__(self, emb_dim):
	super().__init__()	
	self.emb_dim = emb_dim

	self.tokeys = nn.Linear(emb_dim, emb_dim, bias=False)
	self.toqueries = nn.Linear(emb_dim, emb_dim, bias=False)
	self.tovalues = nn.Linear(emb_dim, emb_dim, bias=False)	
  def forward(self, x):
	b, t, dk = x.size()
	queries = self.toqueries(x)
	keys    = self.tokeys(x)
	values  = self.tovalues(x)		
	dot = torch.einsum("bqe, bke -> bqk", [queries, keys])
	softmaxed = torch.softmax(dot / (dk ** (1/2)), dim = 2)
	out = torch.einsum("bqk, bke -> bqe", [softmaxed, values])	
	return out					
					</code></pre>
				</section>
			</section>
			
			<section>
				<section>
					<h3>Multi-Head Attention</h3>
					<p class = "fragment">We just stack several "Scaled Dot-Product Attention" :)</p>
					<p class = "fragment" style="text-align: center">
						Each attention mechanism (indexed by $h$) with different $W_{k}^{h}$, $W_{v}^{h}$, $W^s_{h}$ matrices. 
					</p>
					<p class = "fragment" style="text-align: center">
						For input $x_i$, we produce different output vector $y_i^s$ for each head. 
					</p>
					<p class = "fragment" style="text-align: center">
						We concatenate vectors $y_i^s$ and transform them linearly back to expected dimension.
					</p>
				</section>

				<section>
					<h3>Multi-Head Attention</h3>
					<p>	
						$$\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2,..., \text{head}_h)W_o$$
						$$\text{head}_i = Attention(Q^i, K^i, V^i) $$
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div>
							<br>
							$ Q^i = XW_q^i $ <br> $ K^i = XW_k^i $ <br> $ V^i = XW_v^i $
						</div>
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/multi-head-attention.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Multi-Head Attention</h3>
					<pre data-id="code-animation">
						<code class="python" data-trim data-line-numbers="1-10|11|12-14|15-17|18-19|20|21">
class MultiHeadSelfAttention(nn.Module):
  def __init__(self, emb_dim, heads = 8):
	super().__init__()	
	self.emb_dim = emb_dim
	self.heads = heads
	
	# we combine all heads into single linear transformation
	self.tokeys = nn.Linear(emb_dim, emb_dim * 8, bias=False)
	self.toqueries = nn.Linear(emb_dim, emb_dim * 8, bias=False)
	self.tovalues = nn.Linear(emb_dim, emb_dim * 8 , bias=False)
	self.unifyheads = nn.Linear(emb_dim * 8, emb_dim)	
  def forward(self, x):
	b, t, dk = x.size()
	h = self.heads
	queries = self.toqueries(x).view(b, t, h, dk)
	keys    = self.tokeys(x).view(b, t, h, dk)
	values  = self.tovalues(x).view(b, t, h, dk)	
	dot = torch.einsum("bqhe,bkhe -> bhqk", [queries, keys])
	softmaxed = torch.softmax(dot / (dk ** (1/2)), dim = 3)
	out = torch.einsum("bhqk, bkhe -> bqhe", [softmaxed, values])	
	return self.unifyheads(out)
						</code>
					</pre>
				</section>
			</section>

			<section>
				<section>
					<h3>Position information</h3>
					<p class="fragment">
						Each layer in our model is permutation invariant (no recurrence, no convolution).
						If we change the order of words in a sentence, we get the exact same output.
					</p>
					<p class="fragment">We want out model to make use of the order of the sequence.</p>
				</section>

				<section>
					<h3>Positional Embedding</h3>
					<!-- <p>We simply embed the positions exactly the same way as words.</p> -->
					<p class="fragment">
						Just as we create (learnable) embeddings $v_{\text{cat}}, v_{\text{dog}}$ for words,
						we also create embeddings $v_{1},...,v_{m}$ for each position in a sentence.
					</p>
					<br>
					<p class="fragment">+ easy to implement, works well</p>
					<p class="fragment">- need sequences of every length during training</p>				
				</section>

				<section>
					<h3>Positional Encoding</h3>
					<p class="fragment">
						We use function $f: N \to \mathbb{R}^K$ (usually a combination of $sin$ and $cos$ functions) to create positional embedding for different positions.
					</p>
					<br>
					<p class="fragment">+ works just as well as embeddings</p>
					<p class="fragment">+ deals with sequences longer than in training</p>
					<p class="fragment">- a little harder to implement</p>
					<p class="fragment">- choice of encoding functions can be hard</p>
				</section>
			</section>

			<section>
				<section>
					<h3>Encoder Block</h3>
					<div style="display: inline-grid; grid-template-columns: 60% 40%; margin-left: 0%; margin-right: 0%;">
						<div>
							<ul><br>
								<li class="fragment">Multi-Head Self-Attention</li>
								<li class="fragment">Feed Forward layer - single FC NN applied independently to each vector</li>
								<li class="fragment">Residual connections (added before normalization)</li>
								<li class="fragment">Layer normalization layers</li>
							</ul>
						</div>
						<div style="height: 80%; margin-right: 0%;">
							<img src="assets/encoder.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Let's see it in code!</h3>
				</section>

				<section>
					<h2>Encoder code</h2>
					<pre data-id="code-animation">
						<code class="python" data-trim data-line-numbers="1-4|5|6-7|8-13|14|15|16|17|18">
class Encoder(nn.Module):

    def __init__(self, emb, heads, hidden_multiplier = 4):
        super().__init__()
        self.attention = MultiHeadSelfAttention(emb, heads=heads)
        self.norm1 = nn.LayerNorm(emb)
        self.norm2 = nn.LayerNorm(emb)
        self.ff = nn.Sequential(
            nn.Linear(emb, hidden_multiplier * emb),
            nn.ReLU(),
            nn.Linear(hidden_multiplier * emb, emb)
            )

    def forward(self, x): 
        attended = self.attention(x)
        x = self.norm1(attended + x)
        forwarded = self.ff(x)
        return self.norm2(forwarded + x)
						</code>
					</pre>
				</section>
			</section>

			<section>
				<h2>Encoder-Only architecture examples</h2>
				<p class="fragment"></p>
			</section>

			<section>
				<h2>Simple sequence classification transformer</h2>
				<img src="assets/classifier.svg">
			</section>

			<section>
				<h3>Simple sequence classification transformer</h3>
                <div style="width: 120%">
                    <pre data-id="code-animation">
                        <code class="python" data-trim data-line-numbers>
class ClassificationTransformer(nn.Module):
    def __init__(self, emb, heads, depth, max_seq_length, num_tokens, num_classes):
        super().__init__()

        self.num_tokens = num_tokens
        self.token_emb = nn.Embedding(num_tokens, emb)
        self.pos_emb = nn.Embedding(max_seq_length, emb)

        # stacked transformer blocks
        blocks = [Encoder(emb = emb, heads = heads) for _ in range(depth)]
        self.blocks = nn.Sequential(*blocks)

        # classification on top of transformer blocks
        # (averaged transformer output sequence as input)
        self.toprobs = nn.Linear(emb, num_classes)

    def forward(self, x):
        b, seq_length = x.shape

        # using positional embedding
        # easier to implement than positional encoding
        positions = self.pos_emb(torch.arange(0, max_seq_length, device = 'cuda').expand(b, max_seq_length))
        tokens = self.token_emb(x)

        # entire forward pass with mean
        # averaging the last output sequence
        x = tokens + positions
        x = self.blocks(x)
        x = x.mean(dim=1)
        x = self.toprobs(x)

        return F.log_softmax(x, dim = 1)
                        </code>
                    </pre>
                </div>
            </section>
            
            <section>
                <section>
                    <h3>BERT</h3>
                    <p class="fragment">BERT (largest) = stack of 24 encoder blocks with embedding dimension of 1024 and 16 attention heads.</p>
                    <p class="fragment">It is pretrained on 800M words from English books and 2.5B words from wikipedia.</p>
                    <p class="fragment">Pretraining is done with 2 tasks:</p>
                    <ol>
                        <li class="fragment">Masking</li>
                        <li class="fragment">Next sentence classification</li>
                    </ol>
                </section>

                <section>
                    <h3>Masking</h3>
                    <p class="fragment">We replace randomly 15% of words in the input sequence. The model is then asked to predict the masked words.</p>
                    <p class="fragment">Masking procedure:</p>
                    <ul>
                        <li class="fragment">
                            [MASK] token - 80% of time: <br>
                            my dog is hairy -> my dog is [MASK]
                        </li>
                        <li class="fragment">
                            Random token - 10% of time: <br>
                            my dog is hairy -> mydog is apple
                        </li>
                        <li class="fragment">
                            Unchanged token - 10% of time:
                            my dog is hairy -> mydog is apple
                        </li>
                    </ul>
                </section>

                <section>
                    <h3>Next sequence classification</h3>
                    <p class="fragment">We sample 2 sequences from our corpus that either:</p>
                    <ul>
                        <li class="fragment">Follow each other directly</li>
                        <li class="fragment">Are taken from random places</li>
                    </ul>
                    <p class="fragment">The model is then asked whick of those is the case.</p>
                    <img class="fragment" src="assets/next-sequence-prediction.png" style="width: 60%">
                </section>

                <section>
                    <img src="assets/bert-pretraining.png" style="width:80%">
                </section>
            </section>

            <section>
                <h1>The end!</h1>
            </section>

            <section>
                <h3>References</h3>
                <ul>
                    <li>https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</li>
                    <li>https://www.youtube.com/watch?v=iDulhoQ2pro</li>
                    <li>http://jalammar.github.io/illustrated-transformer/</li>
                    <li>https://lionbridge.ai/articles/what-are-transformer-models-in-machine-learning/</li>
                    <li>http://peterbloem.nl/blog/transformers</li>
                    <li>https://arxiv.org/abs/1810.04805</li>
                    <li>https://arxiv.org/abs/1706.03762</li>
                    <li>https://github.com/BartWojtowicz/transformer-from-scratch</li>
                </ul>
            </section>
			
		<!-- Not finished, leaving decoder for the future presentation

			<section>
				<h2>Back to Vanilla Transformer for a second!</h2>
			</section>

			<section>
				<h3>Decoder</h3>
				<div style="display: inline-grid; grid-template-columns: 40% 60%;margin-left: 0%; margin-right: 0%;">
                    <div style="height: 100%; margin-bottom: 0%;">
                        <img src="assets/vanilla_transformer.png">
                    </div>
                    <div>
						<ul>
							<li class="fragment">Encoder (stack of N) maps the input sequence into a single latent vector (memory).</li>
							<li class="fragment">We use Masked Attention so we can only 'attend' to previous words in the output.</li>
							<li class="fragment">In second attention we use Keys and Values from encoder, and Queries from decoder.</li>
						</ul>
                    </div>
                </div>
			</section>


			<section>
				<h3>Masked Multi-Head Attention</h3>
				<img src='assets/masking.png'>
				<p>Before the softmax operation we set masked out elements to $-\text{inf}$.</p>
				<p></p>
			</section>
			
		-->
			
		</div>
	</div>
	
	<script src="plugin/math/math.js"></script>
	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath  ]
		});
	</script>
</body>
</html>
