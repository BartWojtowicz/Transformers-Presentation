<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Transformers</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/black.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">

</head>
<body>
	<!--
	The plan:
		1. The problem with RNNs (reminder)
		2. First look at vanilla transformer
		3. Self Attention
		4. Transformer Block
		5. Attention is all you need vanilla transformer
		6. Encoder only 
		7. Examples
			7.1 Basic classification
			7.2 Bert 
	-->

	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Transformer</h2>
				<p>Architecture overview</p>
				<small>Bartosz WÃ³jtowicz</small>
			</section>

			<section>
				<h2>Prerequisites</h2>
				<ul>
					<li class="fragment">You know how Neural Networks work</li>
					<li class="fragment">Basic knowledge of PyTorch</li>
					<li class="fragment">(optional) Basic understanding of attention mechanism</li>
				</ul>
			</section>

			<section data-auto-animate>
				<h2 data-id='code-title'>Presentation outline</h2>
				<ol>
					<li class = "fragment">The problem with RNNs (quick recap)</li>
					<li class = "fragment">Quick look at Vanilla Transformer - "Attention is All You Need" paper</li>
					<li class = "fragment">Vanilla Transformer - architecture breakdown</li>
					<li class = "fragment">Examples of architectures</li>
				</ol>
			</section>

			<section>
				<h3>The problem with RNNs</h3>
				<img src = "assets/recurrent-connection.svg" style="height: 10%;">
			</section>

			<section>
				<h3>RNN problems</h3>
				<ul>
					<li>Not parallelizable within a sequence - RNNs process tokens sequentially</li>
					<li>Problems with (very) long dependencies</li>
					<li>Vanishing/exploding gradients</li>
					<li>Computationally expensive to train</li>
				</ul>
			</section>

			<section>
				<h3>Vanilla Transformer</h3>
				<div style="display: inline-grid; grid-template-columns: 50% 50%;margin-left: 0%; margin-right: 0%;">
                    <div style="height: 85%; margin-bottom: 0%;">
                        <img src="assets/vanilla_transformer.png">
                    </div>
                    <div>
						<p>We'll break it down into:</p>
						<ol>
							<li class = "fragment">Self-Attention</li>
							<li class = "fragment">Multi-Head Attention</li>
							<li class = "fragment">Position encodings and embeddings</li>
							<li class = "fragment">Encoder Block</li>
							<li class = "fragment">Decoder Block</li>
						</ol>
                    </div>
                </div>
			</section>

			<section>
				<section>	
					<h3>Self-Attention - basic example</h3>
					<p>
						We are transforming input vectors $x_1, x_2,..., x_t$ into corrseponding  output vectors $y_1, y_2,..., y_t$.
						Here $y_i$ is simply weighed avereage over all input vectors.
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div style="height: 120%; margin-right: 0%;">
							<img src="assets/self-attention-idea.svg">
						</div>
						<div>
							<p>
								$y_i = \sum_{j} w_{ij}x_j$ <br><br>
								$w'_{ij} = x_{i}^{T}x_j$ <br><br>
								$w_{ij} = \text{softmax}(w'_{ij})$
							</p>
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - key, value, query</h3>
					<p style="text-align: left">
						From each input vector $x_i$ we derive three new vectors called $key, value$ and $query$. <br>					
					</p>
					<div style="display: inline-grid; grid-template-columns: 60% 40%; margin-left: 0%; margin-right: 0%;">
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/key-query-value.svg">
						</div>
						<div>
							$k_i = W_kx_i$ <br> $v_i = W_vx_i$ <br> $q_i = W_qx_i$ <br><br>
							$y_i = \sum_{j} w_{ij}x_j$ <br><br>
							$w'_{ij} = q_{i}^{T}k_j$ <br>
							$w_{ij} = \text{softmax}(w'_{ij})$
						</div>
					</div>
				</section>

				<section>
					<h3>Self-Attention - Scaling dot product</h3>
					<p>
						The next trick is to scale our dot product by $\frac{1}{\sqrt{d_k}}$, where $d_k$ is the length of 
						our embedding vector. 
					</p><br>
					$y_i = \sum_{j} w_{ij}x_j$ <br>
					$w'_{ij} = \frac{q_{i}^{T}k_j}{\sqrt{d_k}}$ <br>
					$w_{ij} = \text{softmax}(w'_{ij})$
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<div style="display: inline-grid; grid-template-columns: 65% 35%; margin-left: 0%; margin-right: 0%;">
						<div>
							<p>
								Keys, values and queries are packed into matrices $K, V, Q$, which 
								enables the attention function to be computed simultaneously over set of queries.
							</p>
						</div>
						<div style="height: 100%; margin-right: 0%;">
							<img src="assets/scaled-dotprod-attention.png">
						</div>
					</div>
				</section>

				<section>
					<h3>Scaled Dot-Product Attention</h3>
					<p>
						$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ 
					</p>
					<pre data-id="code-animation"><code class="python" data-trim data-line-numbers="1-8|10|11-13|14-15|16">
class SelfAttention(nn.Module):
  def __init__(self, emb_dim):
	super().__init__()	
	self.emb_dim = emb_dim

	self.tokeys = nn.Linear(emb_dim, emb_dim, bias=False)
	self.toqueries = nn.Linear(emb_dim, emb_dim, bias=False)
	self.tovalues = nn.Linear(emb_dim, emb_dim, bias=False)	
  def forward(self, x):
	b, t, dk = x.size()
	queries = self.toqueries(x)
	keys    = self.tokeys(x)
	values  = self.tovalues(x)		
	dot = torch.einsum("bqe,bke -> bqk", [queries, keys])
	softmaxed = torch.softmax(dot / (dk ** (1/2)), dim = 2)
	out = torch.einsum("bqk, bke -> bqe", [softmaxed, values])	
	return out					
					</code></pre>
				</section>
			</section>

			<section>
				<h3>What about the "Multi-head" thing?</h3>
			</section>
			
		</div>
	</div>
	
	<script src="plugin/math/math.js"></script>
	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath  ]
		});
	</script>
</body>
</html>
